{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Model\n",
        "This notebook demonstrates implementing logistic regression, including data processing, gradient descent optimization, and loss calculation."
      ],
      "metadata": {
        "id": "AjdM9_FFEft4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "NO2oQu9BEzvJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Functions"
      ],
      "metadata": {
        "id": "mGcW2i9eE-Ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sigmoid Function\n",
        "The sigmoid activation function, which converts input values into probabilities."
      ],
      "metadata": {
        "id": "Q88lpP3WFPr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "dWGVNRhgFTGZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function\n",
        "Computes the cross-entropy loss, which measures the difference between predicted values and true labels."
      ],
      "metadata": {
        "id": "ukOgkuQpFZDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(y, t):\n",
        "    return -np.mean(t * np.log(y) + (1 - t) * np.log(1 - y))"
      ],
      "metadata": {
        "id": "2_n3F-wwFd9G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Parameters\n",
        "The model parameters, including weights w and the learning rate."
      ],
      "metadata": {
        "id": "mi8H65yIFhAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.array([-0.6, 2, 1.2, -2.8])\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "CoYkf_QmFmnd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset\n",
        "Input dataset X and target labels t."
      ],
      "metadata": {
        "id": "qY2snBUxFp1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([\n",
        "    [0, 0, 0],\n",
        "    [0, 1, 0],\n",
        "    [1, 0, 0],\n",
        "    [1, 0, 0]\n",
        "])\n",
        "\n",
        "t = np.array([0, 1, 1, 1])\n"
      ],
      "metadata": {
        "id": "Rq9EC3F0FtpT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        "Using gradient descent to optimize w while storing the last 10 updates."
      ],
      "metadata": {
        "id": "8ZZAG-MCFzVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 1000\n",
        "last_updates = []\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    # Compute z and y\n",
        "    z = np.dot(X, w[1:]) + w[0]\n",
        "    y = sigmoid(z)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = compute_loss(y, t)\n",
        "\n",
        "    # Compute gradients\n",
        "    dw0 = np.mean(y - t)\n",
        "    dw1 = np.mean((y - t) * X[:, 0])\n",
        "    dw2 = np.mean((y - t) * X[:, 1])\n",
        "    dw3 = np.mean((y - t) * X[:, 2])\n",
        "\n",
        "    # Update weights\n",
        "    w[0] -= learning_rate * dw0\n",
        "    w[1] -= learning_rate * dw1\n",
        "    w[2] -= learning_rate * dw2\n",
        "    w[3] -= learning_rate * dw3\n",
        "\n",
        "    # Store the last 10 updates\n",
        "    if i >= num_iterations - 10:\n",
        "        last_updates.append((z, y, loss))"
      ],
      "metadata": {
        "id": "6aa5BIytF3jj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "Prints the last 10 updates of z, y, and loss."
      ],
      "metadata": {
        "id": "uDvd1nZeGBAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for update in last_updates:\n",
        "    print(f\"z: {update[0]}, y: {update[1]}, Loss: {update[2]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOErNh8NGGit",
        "outputId": "bcea2a79-902d-41f7-bc04-2af5890ee7c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z: [-1.58281443  2.61858922  3.37245296  3.37245296], y: [0.17039726 0.93204841 0.96683244 0.96683244], Loss: 0.08115974786115632\n",
            "z: [-1.5837172   2.61938525  3.37320857  3.37320857], y: [0.17026968 0.93209881 0.96685666 0.96685666], Loss: 0.08109526098664563\n",
            "z: [-1.58461924  2.62018073  3.37396369  3.37396369], y: [0.17014228 0.93214914 0.96688085 0.96688085], Loss: 0.08103086997390248\n",
            "z: [-1.58552057  2.62097568  3.37471832  3.37471832], y: [0.17001505 0.9321994  0.96690501 0.96690501], Loss: 0.08096657461953848\n",
            "z: [-1.58642118  2.62177008  3.37547246  3.37547246], y: [0.16988801 0.93224959 0.96692913 0.96692913], Loss: 0.08090237472071676\n",
            "z: [-1.58732108  2.62256394  3.37622611  3.37622611], y: [0.16976114 0.93229971 0.96695322 0.96695322], Loss: 0.08083827007514942\n",
            "z: [-1.58822026  2.62335727  3.37697926  3.37697926], y: [0.16963444 0.93234977 0.96697728 0.96697728], Loss: 0.08077426048109682\n",
            "z: [-1.58911873  2.62415006  3.37773193  3.37773193], y: [0.16950792 0.93239976 0.96700131 0.96700131], Loss: 0.08071034573736446\n",
            "z: [-1.59001649  2.6249423   3.37848411  3.37848411], y: [0.16938158 0.93244967 0.9670253  0.9670253 ], Loss: 0.08064652564330219\n",
            "z: [-1.59091353  2.62573402  3.3792358   3.3792358 ], y: [0.16925541 0.93249953 0.96704926 0.96704926], Loss: 0.08058279999880222\n"
          ]
        }
      ]
    }
  ]
}